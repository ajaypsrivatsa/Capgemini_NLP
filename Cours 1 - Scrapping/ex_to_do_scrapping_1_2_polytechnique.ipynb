{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false},"colab":{"name":"ex_to_do_scrapping_1_2_polytechnique.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"tiHeGF3nesy3"},"source":["# Scrapping Exercises - Part 1 - 2\n","\n","You just had you first scrapping experience using Selectors on Capgemini's webpages to make some parsing on it.\n","\n","As we mentionned earlier, scrapping has two parts : \n","- Parsing : getting information from a webpage\n","- Crawling : traveling from pages to pages\n","\n","Here we'are again going to have a parsing experience but on a much more `scrapable`page. You'll soon understand why ;) !"]},{"cell_type":"markdown","metadata":{"id":"tzKZbi9Resy8"},"source":["## 1. Make necessary imports"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g9PRViQvesy9","executionInfo":{"status":"ok","timestamp":1611317281026,"user_tz":-60,"elapsed":16450,"user":{"displayName":"ismail mebsout","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GigU0PnKefD-R2lR5GVCQk1yGIr32KHGeWyGF1k=s64","userId":"12097835266700298787"}},"outputId":"27165230-8ed7-4f59-e4a4-e8fe1786a0bb"},"source":["import requests\n","%pip install scrapy\n","from scrapy.selector import Selector"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting scrapy\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/16/3c7c37caf25f91aa21db194655515718c2a15f704f9f5c59a194f5c83db0/Scrapy-2.4.1-py2.py3-none-any.whl (239kB)\n","\u001b[K     |████████████████████████████████| 245kB 8.9MB/s \n","\u001b[?25hCollecting cssselect>=0.9.1\n","  Downloading https://files.pythonhosted.org/packages/3b/d4/3b5c17f00cce85b9a1e6f91096e1cc8e8ede2e1be8e96b87ce1ed09e92c5/cssselect-1.1.0-py2.py3-none-any.whl\n","Collecting pyOpenSSL>=16.2.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b2/5e/06351ede29fd4899782ad335c2e02f1f862a887c20a3541f17c3fa1a3525/pyOpenSSL-20.0.1-py2.py3-none-any.whl (54kB)\n","\u001b[K     |████████████████████████████████| 61kB 5.2MB/s \n","\u001b[?25hCollecting zope.interface>=4.1.3\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/82/b0/da8afd9b3bd50c7665ecdac062f182982af1173c9081f9af7261091c5588/zope.interface-5.2.0-cp36-cp36m-manylinux2010_x86_64.whl (236kB)\n","\u001b[K     |████████████████████████████████| 245kB 11.8MB/s \n","\u001b[?25hCollecting itemadapter>=0.1.0\n","  Downloading https://files.pythonhosted.org/packages/88/83/ab33780fd93278e699561d61862d27343c95d3fe0a0081acd73e8e26a649/itemadapter-0.2.0-py3-none-any.whl\n","Collecting itemloaders>=1.0.1\n","  Downloading https://files.pythonhosted.org/packages/b3/2b/eb2ddf7becf834679273a6f79ffdc6fbedf07c5272e2eddf412582143c0e/itemloaders-1.0.4-py3-none-any.whl\n","Collecting PyDispatcher>=2.0.5\n","  Downloading https://files.pythonhosted.org/packages/cd/37/39aca520918ce1935bea9c356bcbb7ed7e52ad4e31bff9b943dfc8e7115b/PyDispatcher-2.0.5.tar.gz\n","Collecting service-identity>=16.0.0\n","  Downloading https://files.pythonhosted.org/packages/e9/7c/2195b890023e098f9618d43ebc337d83c8b38d414326685339eb024db2f6/service_identity-18.1.0-py2.py3-none-any.whl\n","Collecting queuelib>=1.4.2\n","  Downloading https://files.pythonhosted.org/packages/4c/85/ae64e9145f39dd6d14f8af3fa809a270ef3729f3b90b3c0cf5aa242ab0d4/queuelib-1.5.0-py2.py3-none-any.whl\n","Collecting w3lib>=1.17.0\n","  Downloading https://files.pythonhosted.org/packages/a3/59/b6b14521090e7f42669cafdb84b0ab89301a42f1f1a82fcf5856661ea3a7/w3lib-1.22.0-py2.py3-none-any.whl\n","Collecting Twisted>=17.9.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b7/04/1a664c9e5ec0224a1c1a154ddecaa4dc7b8967521bba225efcc41a03d5f3/Twisted-20.3.0-cp36-cp36m-manylinux1_x86_64.whl (3.1MB)\n","\u001b[K     |████████████████████████████████| 3.1MB 14.7MB/s \n","\u001b[?25hCollecting protego>=0.1.15\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/db/6e/bf6d5e4d7cf233b785719aaec2c38f027b9c2ed980a0015ec1a1cced4893/Protego-0.1.16.tar.gz (3.2MB)\n","\u001b[K     |████████████████████████████████| 3.2MB 24.4MB/s \n","\u001b[?25hRequirement already satisfied: lxml>=3.5.0; platform_python_implementation == \"CPython\" in /usr/local/lib/python3.6/dist-packages (from scrapy) (4.2.6)\n","Collecting cryptography>=2.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c9/de/7054df0620b5411ba45480f0261e1fb66a53f3db31b28e3aa52c026e72d9/cryptography-3.3.1-cp36-abi3-manylinux2010_x86_64.whl (2.6MB)\n","\u001b[K     |████████████████████████████████| 2.6MB 44.2MB/s \n","\u001b[?25hCollecting parsel>=1.5.0\n","  Downloading https://files.pythonhosted.org/packages/23/1e/9b39d64cbab79d4362cdd7be7f5e9623d45c4a53b3f7522cd8210df52d8e/parsel-1.6.0-py2.py3-none-any.whl\n","Requirement already satisfied: six>=1.5.2 in /usr/local/lib/python3.6/dist-packages (from pyOpenSSL>=16.2.0->scrapy) (1.15.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from zope.interface>=4.1.3->scrapy) (51.3.3)\n","Collecting jmespath>=0.9.5\n","  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n","Requirement already satisfied: attrs>=16.0.0 in /usr/local/lib/python3.6/dist-packages (from service-identity>=16.0.0->scrapy) (20.3.0)\n","Requirement already satisfied: pyasn1-modules in /usr/local/lib/python3.6/dist-packages (from service-identity>=16.0.0->scrapy) (0.2.8)\n","Requirement already satisfied: pyasn1 in /usr/local/lib/python3.6/dist-packages (from service-identity>=16.0.0->scrapy) (0.4.8)\n","Collecting hyperlink>=17.1.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6e/aa/8caf6a0a3e62863cbb9dab27135660acba46903b703e224f14f447e57934/hyperlink-21.0.0-py2.py3-none-any.whl (74kB)\n","\u001b[K     |████████████████████████████████| 81kB 9.7MB/s \n","\u001b[?25hCollecting constantly>=15.1\n","  Downloading https://files.pythonhosted.org/packages/b9/65/48c1909d0c0aeae6c10213340ce682db01b48ea900a7d9fce7a7910ff318/constantly-15.1.0-py2.py3-none-any.whl\n","Collecting Automat>=0.3.0\n","  Downloading https://files.pythonhosted.org/packages/dd/83/5f6f3c1a562674d65efc320257bdc0873ec53147835aeef7762fe7585273/Automat-20.2.0-py2.py3-none-any.whl\n","Collecting PyHamcrest!=1.10.0,>=1.9.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/16/e54cc65891f01cb62893540f44ffd3e8dab0a22443e1b438f1a9f5574bee/PyHamcrest-2.0.2-py3-none-any.whl (52kB)\n","\u001b[K     |████████████████████████████████| 61kB 8.2MB/s \n","\u001b[?25hCollecting incremental>=16.10.1\n","  Downloading https://files.pythonhosted.org/packages/f5/1d/c98a587dc06e107115cf4a58b49de20b19222c83d75335a192052af4c4b7/incremental-17.5.0-py2.py3-none-any.whl\n","Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.6/dist-packages (from cryptography>=2.0->scrapy) (1.14.4)\n","Requirement already satisfied: idna>=2.5 in /usr/local/lib/python3.6/dist-packages (from hyperlink>=17.1.1->Twisted>=17.9.0->scrapy) (2.10)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi>=1.12->cryptography>=2.0->scrapy) (2.20)\n","Building wheels for collected packages: PyDispatcher, protego\n","  Building wheel for PyDispatcher (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for PyDispatcher: filename=PyDispatcher-2.0.5-cp36-none-any.whl size=11516 sha256=e75b346a957f602d953ab51e6435175a5b7f985c10cc612502f0d2a3bc43d29b\n","  Stored in directory: /root/.cache/pip/wheels/88/99/96/cfef6665f9cb1522ee6757ae5955feedf2fe25f1737f91fa7f\n","  Building wheel for protego (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for protego: filename=Protego-0.1.16-cp36-none-any.whl size=7766 sha256=5a93bf561561927fbf95f2aebe0a760dbbf1d035306b4d50a310635b50457b98\n","  Stored in directory: /root/.cache/pip/wheels/51/01/d1/4a2286a976dccd025ba679acacfe37320540df0f2283ecab12\n","Successfully built PyDispatcher protego\n","Installing collected packages: cssselect, cryptography, pyOpenSSL, zope.interface, itemadapter, jmespath, w3lib, parsel, itemloaders, PyDispatcher, service-identity, queuelib, hyperlink, constantly, Automat, PyHamcrest, incremental, Twisted, protego, scrapy\n","Successfully installed Automat-20.2.0 PyDispatcher-2.0.5 PyHamcrest-2.0.2 Twisted-20.3.0 constantly-15.1.0 cryptography-3.3.1 cssselect-1.1.0 hyperlink-21.0.0 incremental-17.5.0 itemadapter-0.2.0 itemloaders-1.0.4 jmespath-0.10.0 parsel-1.6.0 protego-0.1.16 pyOpenSSL-20.0.1 queuelib-1.5.0 scrapy-2.4.1 service-identity-18.1.0 w3lib-1.22.0 zope.interface-5.2.0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"CL_Dce45esy-"},"source":["## 2. Get some information from Ecole Polytechniques's website\n","\n","This second exercice is closer to a scrapping approach. In the next parse of the course you will learn how to go from pages to pages.\n","In order to be able, we'll ask you to get some information from [this page](https://www.polytechnique.edu/fr/actualités)."]},{"cell_type":"code","metadata":{"id":"0_Vg5nxNesy-"},"source":["# Connection to Polytechnique actu page\n","url = 'https://www.polytechnique.edu/fr/actualités'\n","html = requests.get(url).content\n","sel = Selector(text=html)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N35dut6Fesy-"},"source":["__<font color='Blue'>\n","Exercice 1 : Using selectors, extract the dates of all the articles of the page\n","</font>__"]},{"cell_type":"code","metadata":{"id":"Yg75ie39esy-"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5U2bUDvMesy_"},"source":["__<font color='Blue'>\n","Exercice 2 : Using selectors, extract the links of all the articles of the page\n","</font>__"]},{"cell_type":"code","metadata":{"id":"l3UHPiI4eszJ"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mIRuhP8YeszK"},"source":["__<font color='Blue'>\n","Exercice 3 : Using selectors, extract the link of the fllowing page of articles\n","</font>__"]},{"cell_type":"code","metadata":{"id":"WTnuazV6eszL"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DSve-zdQeszM"},"source":["## 3. Doing the same article by article\n","\n","The purpose of this part is to get closer of a scrapping approach.\n","Now what you are going to do is :\n","- Get a list of hmtl objects\n","- And parse them one by one"]},{"cell_type":"markdown","metadata":{"id":"KzZPw6sBeszM"},"source":["__<font color='Blue'>\n","Exercice 4 : Using selectors, get a list of the articles of the page\n","</font>__"]},{"cell_type":"code","metadata":{"id":"9JjZ8TdXeszM"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nY2NodMieszM"},"source":["__<font color='Blue'>\n","Exercice 5 : For a given article, extract the url of the dedicated webapge\n","</font>__"]},{"cell_type":"code","metadata":{"id":"7J5XqNsVeszN"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1eUtJ1mPeszN"},"source":["__<font color='Blue'>\n","Exercice 6 : For a given article, extract the title and date\n","</font>__\n","\n","_hint : you can do it with a single selector_"]},{"cell_type":"code","metadata":{"id":"pjbA6ae3eszN"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3AE6qHgmeszO"},"source":["__<font color='Blue'>\n","Exercice 7 : For a given article, extract the short description related to it\n","</font>__"]},{"cell_type":"code","metadata":{"id":"CSnhN-J9eszO"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7EaCjRjAeszO"},"source":["__<font color='Blue'>\n","Exercice 8 : For a given article, extract the #hashtags mentionning related topics\n","</font>__\n"]},{"cell_type":"code","metadata":{"id":"TphTlh3AeszO"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4ZaXK5g3eszO"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ECyg_Tk8eszP"},"source":["## 5. (Bonus) Looking at an Article page \n","\n","\n","(this is quite similar but allows us to get all the content of the article)"]},{"cell_type":"code","metadata":{"id":"CNrL1JuLeszP"},"source":["# Getting the selector\n","url = 'https://www.polytechnique.edu/fr/content/de-nouvelles-experiences-au-cern'\n","html = requests.get(url).content\n","sel = Selector(text=html)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oYzKwQSteszP"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"j5HfC9AEeszP"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6OsHHK09eszQ"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uHEuhbZYeszQ"},"source":["## Congratulations !\n","\n","__Now that you have all the selectors, you can think of a spider using them__\n","- As there are many wat to get a selectors, yours might be different to ours.\n","- You can try to replace them and re run the spider ;) !"]},{"cell_type":"code","metadata":{"id":"3b_mMvWteszQ"},"source":[""],"execution_count":null,"outputs":[]}]}