{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"colab":{"name":"embedding_1LSI_hands_on.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"ubAzQDWb5KdO"},"source":["# LSA Demonstrator\n","In this tutorial, you will learn how to use Latent Semantic Analysis to either discover hidden topics from given documents in an unsupervised way \n","Later you'll use LSA values as a feature vectors to classify document with known document categories."]},{"cell_type":"markdown","metadata":{"id":"jro0Xf3P5KdQ"},"source":["## Imports"]},{"cell_type":"code","metadata":{"id":"lqpmbyPs53bo"},"source":["# Load file from drive\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"l6qKUbGC54CR"},"source":["import os\n","os.chdir(\"drive/MyDrive/NLP @ X_HEC - 2K21/Cours 3 - Embedding part 1/\") # path to your drive folder"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JNeHyIpo553A"},"source":["!pwd\n","!ls"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8WYNdqXo5KdR"},"source":["!pip3 install nltk"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZE_7xCgy5KdS"},"source":["#import modules\n","import os\n","import pandas as pd\n","import numpy as np\n","from string import punctuation\n","\n","import nltk\n","from nltk import WordNetLemmatizer, word_tokenize\n","from nltk.corpus import stopwords\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.decomposition import TruncatedSVD\n","import matplotlib.pyplot as plt\n","\n","nltk.download(\"stopwords\")\n","nltk.download('punkt')\n","nltk.download(\"wordnet\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IFIRFwM15KdT"},"source":["## Preprocessing function"]},{"cell_type":"code","metadata":{"id":"bA_s5aKo5KdT"},"source":["stop_words = nltk.corpus.stopwords.words(\"english\")\n","stop_char = stop_words + list(punctuation)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DDlWidVV5KdT"},"source":["def preprocessing(sentence):\n","    \"\"\" Basic processing of a document, word by word. \n","    Outputs a list of processed tokens\n","    \"\"\"\n","    # Tokenization\n","    tokens = word_tokenize(sentence)\n","    # stopwords + lowercase\n","    tokens = [token.lower().replace(\"'\", \"\") for token in tokens if token.lower() not in stop_char]\n","    \n","    Lemmatizer = WordNetLemmatizer()\n","    tokens = [Lemmatizer.lemmatize(token) for token in tokens]\n","    \n","    # Deleting words with  only one caracter\n","    tokens = [token for token in tokens if len(token)>2]\n","    \n","    return tokens"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"slasCb9Q5KdU"},"source":["## Work on your data !"]},{"cell_type":"code","metadata":{"id":"c_y0atBw5KdU"},"source":["## import your cleaned data\n","path_to_your_dataset = 'data/clean_text_scrapped_data_2021.csv'\n","\n","reviews = pd.read_csv(path_to_your_dataset,\n","            low_memory=False,\n","            parse_dates=['rating_date', 'diner_date']\n","            )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B296vVUa5KdU"},"source":["### Preprocessing"]},{"cell_type":"code","metadata":{"id":"QKre_l3a5Kda"},"source":["## apply preprocessing on each document (optional)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HzqT9kML5Kdh"},"source":["### TF-IDF vectorization\n","To convert text data in a document-term matrix, we are goint to use `TfidfVectorizer` from `sklearn` library"]},{"cell_type":"code","metadata":{"id":"Ab-7N1pq5Kdj"},"source":["## Build TF-idf matrix from your data\n","## terms in columns, document in rows\n","\n","#dictionary = np.array(vectorizer.get_feature_names())\n","#df_tfidf = pd.DataFrame(vect_corpus.todense(), columns = dictionary)\n","#df_tfidf.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TCR4tqo75Kdk"},"source":["### Singular Value Decomposition"]},{"cell_type":"markdown","metadata":{"id":"m3uvyh6J5Kdk"},"source":["To perform Singular Value Decomposition, you can use `TruncatedSVD`. You must specify the number of topics/latent features you are expecting. Default value is set to 2. Here we will keep 2 as number of components as we are expecting to discover 2 topics regarding this corpus. Later, you'll see how to optimize this number.\n","Keep in mind that your latent features are sorted by decreasingly importance."]},{"cell_type":"code","metadata":{"id":"XMpE6w9D5Kdk"},"source":["## Build Singular Value Decomposition using TruncatedSVD. You can choose the number of components you want to use.\n","\n","#n_components = 5"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2Cib7r035Kdl"},"source":["#Convert your lsa in document_concept matrix\n","\n","#topic_encoded_df = pd.DataFrame(lsa, columns=[f'topic_{i+1}' for i in range(n_components)])\n","#topic_encoded_df['corpus'] = corpus\n","#topic_encoded_df"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"moNBi-H_5Kdl"},"source":["### Deep dive into dictionary"]},{"cell_type":"markdown","metadata":{"id":"AR4unKyS5Kdl"},"source":["Use the `components_`attribute of svd to get your term_concept similarity matrix"]},{"cell_type":"code","metadata":{"id":"waIfJ5zG5Kdm"},"source":["#encoding_matrix = pd.DataFrame(svd.components_, index=[f'topic_{i+1}' for i in range(n_components)], columns=dictionary).T\n","#encoding_matrix"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"x9vQK-yL5Kdn"},"source":["Have a look to the top words of each topic (think about the absolute value)"]},{"cell_type":"code","metadata":{"id":"HcAmqfVH5Kdn"},"source":["## top words by topics"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WK6Ypfe55Kdn"},"source":["### Plot topic encoded data"]},{"cell_type":"markdown","metadata":{"id":"ZQYwkWI55Kdo"},"source":["We are going to represent each sentence regarding the first two latent features.\n","You can use rating of the review as target, to marked each of your document."]},{"cell_type":"code","metadata":{"id":"zht570yj5Kdo"},"source":["fig, ax = plt.subplots(figsize=(10,10))\n","\n","colors = ['red', 'orange', 'yellow', 'green', 'blue']\n","\n","for val in topic_encoded_df['rating'].unique():\n","    topic_1 = topic_encoded_df[topic_encoded_df['rating']==val]['topic_1'].values\n","    topic_2 = topic_encoded_df[topic_encoded_df['rating']==val]['topic_2'].values\n","    color = colors[i]\n","    ax.scatter(topic_1, topic_2, alpha=0.5, label=val, color=color)\n","    \n","ax.set_xlabel('First Topic')\n","ax.set_ylabel('Second Topic')\n","ax.axvline(linewidth=0.5)\n","ax.axhline(linewidth=0.5)\n","ax.legend()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1gHYiYzI5Kdo"},"source":["## Select the best number of components for SVD"]},{"cell_type":"markdown","metadata":{"id":"GulbvFsX5Kdo"},"source":["Create a function calculating the number of components required to pass threshold. \n","This function has to take in parameters a large list of explained variance ratio (number of components close from number of originally features/terms). You can use the `explained_variance_ratio_` attribute of your svd"]},{"cell_type":"code","metadata":{"id":"5G38pORD5Kdp"},"source":["def select_n_components(var_ratio, var_threshold):\n","    # Set initial variance explained explained_variance\n","    \n","    # Set initial number of features n_components\n","    \n","    # For the explained variance of each feature:\n","    for explained_variance in var_ratio:\n","        \n","        # Add the explained variance to the total\n","        \n","        # Add one to the number of components\n","        \n","        # If we reach our goal level of explained variance\n","        if  >= :\n","            # End the loop\n","            break\n","            \n","    # Return the number of components\n","    return n_components"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2A40ejKf5Kdp"},"source":["Now, perform LSA with large number of components (one less than number of features of your input matrix) and then use your fonction to find a good number of components"]},{"cell_type":"code","metadata":{"id":"fRpicLiP5Kdr"},"source":["large_svd = TruncatedSVD(n_components=df_tfidf.shape[1]-1)\n","large_lsa = large_svd.fit_transform(df_tfidf)\n","threshold = 0.5"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5NUs35Ia5Kdr"},"source":["fig, ax = plt.subplots(figsize=(10,10))\n","\n","explained_variance = pd.Series(large_svd.explained_variance_ratio_.cumsum())\n","explained_variance.plot()\n","\n","ax.xaxis.set_ticks(np.arange(0, len(explained_variance), 100))\n","\n","ax.set_xlabel('Number of Topics')\n","ax.set_ylabel('Percentage of explained variance')\n","ax.set_title('Percentage of explained variance by number of topics')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"X3nDma5w5Kds"},"source":["n_opt = select_n_components(large_svd.explained_variance_ratio_, threshold)\n","print(f\"The optimal number of components to explain {threshold*100}% of the variance is {n_opt}\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gBwaF5mu5Kdt"},"source":["optimal_svd = TruncatedSVD(n_components=n_opt)\n","optimal_lsa = optimal_svd.fit_transform(df_tfidf)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AtlxpH-J5Kdu"},"source":["optimal_encoding_matrix = pd.DataFrame(optimal_svd.components_, index=[f'topic_{i+1}' for i in range(n_opt)], columns=dictionary).T"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iJN856DH5Kdw"},"source":["for i in range(10):\n","    optimal_encoding_matrix[f'abs_topic_{i+1}'] = np.abs(optimal_encoding_matrix[f'topic_{i+1}'])\n","    top_words = optimal_encoding_matrix.sort_values(f'abs_topic_{i+1}', ascending=False).index[:5]\n","    print(f\"Top words for topic {i+1} are : \")\n","    print(top_words)\n","    print()\n","    print()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XpFmXP5B5Kdw"},"source":[""],"execution_count":null,"outputs":[]}]}