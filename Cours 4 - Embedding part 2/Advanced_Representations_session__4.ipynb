{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Advanced_Representations_session__4.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"},"widgets":{"application/vnd.jupyter.widget-state+json":{"5db48e768079410d8be1adf46bd1af13":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_6591e4d7d0834a53aa7ca5c670402f82","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_fe3c5c77c06248499ced64c7fd0fed04","IPY_MODEL_0478d33c46314cf49a7ed79cb376098b"]}},"6591e4d7d0834a53aa7ca5c670402f82":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"fe3c5c77c06248499ced64c7fd0fed04":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_21bd1e83c6984416a8fe04412d08f897","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":0,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_15d4531052ee4f2c9941b98fcb391f86"}},"0478d33c46314cf49a7ed79cb376098b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_604b3b9e4c2646dcadddfda6a1fbebb3","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 0/? [00:00&lt;?, ?it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_1163bbc4cbb84d4cbedbcf1ae3d4442f"}},"21bd1e83c6984416a8fe04412d08f897":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"15d4531052ee4f2c9941b98fcb391f86":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"604b3b9e4c2646dcadddfda6a1fbebb3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"1163bbc4cbb84d4cbedbcf1ae3d4442f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"53d3f0760c2d45238721a91bdb6fde07":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_04bc36de8c7c481cba7bacb044f8cd33","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_318ec404204748638afdb7d67382b75b","IPY_MODEL_c0583191fa9540be85d27cefd11ad3b1"]}},"04bc36de8c7c481cba7bacb044f8cd33":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"318ec404204748638afdb7d67382b75b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_8cbc4c3c980645879de188a7b44cf14d","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":0,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_bb411be2f4ef4019985ca74babbe2aad"}},"c0583191fa9540be85d27cefd11ad3b1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_8308e7be8cd04e7cb9794fb259c37aa1","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 0/? [00:00&lt;?, ?it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_2126670a2a684c9bbbcc68fa4dfb275d"}},"8cbc4c3c980645879de188a7b44cf14d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"bb411be2f4ef4019985ca74babbe2aad":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"8308e7be8cd04e7cb9794fb259c37aa1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"2126670a2a684c9bbbcc68fa4dfb275d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"id":"I_khmHyjMIFj"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jIozR3MQXR-Z"},"source":["# NLP X - HEC Embeddings Part 2 : Advanced Representations\n","\n","\n","In this practical session, we will focus on word embeddings through word2vec, a simple and more advanced classification models for sentiment analysis (reviews ratings prediction). Once a negative sampling word2vec skipgram is trained, we can visualize learned word vectors in a reduced space and use them in our classification model.\n","\n","You will be asked to :\n","1. **Train your own word embeddings with Skipgram** and Tensorflow using **Negative Sampling** method\n","2. **Train a sentiment model** using your pre-trained word embeddings as inputs for a simple classification architecture\n","3. **Upgrade your sentiment model with attention mechanism** through a Hierarchical Attention Network (HAN)\n","4. **Visualize attention weights** to interpret main sentences and words involved in the prediction\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TTopgfVKXc6S","executionInfo":{"status":"ok","timestamp":1614589067992,"user_tz":-60,"elapsed":17840,"user":{"displayName":"ismail mebsout","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GigU0PnKefD-R2lR5GVCQk1yGIr32KHGeWyGF1k=s64","userId":"12097835266700298787"}},"outputId":"d587dfce-7167-48f8-de11-7c4cbb15e4db"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-T128V6_AMxa"},"source":["import os\n","dirpath = os.chdir(\"drive/MyDrive/NLP @ X_HEC - 2K21/Cours 4 - Embedding part 2/\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gTAi_ycvAM9C","executionInfo":{"status":"ok","timestamp":1614589087421,"user_tz":-60,"elapsed":953,"user":{"displayName":"ismail mebsout","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GigU0PnKefD-R2lR5GVCQk1yGIr32KHGeWyGF1k=s64","userId":"12097835266700298787"}},"outputId":"3dcb5d41-b59e-46f1-9385-269bf42fc5ad"},"source":["!pwd\n","!ls"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/drive/MyDrive/NLP @ X_HEC - 2K21/Cours 4 - Embedding part 2\n","Advanced_Representations_session__4.ipynb  data  han.png\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"NhrXXQNjXR-f"},"source":["### Data collection and preprocessing"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PPUEtBV2XR-d","executionInfo":{"status":"ok","timestamp":1614589091089,"user_tz":-60,"elapsed":512,"user":{"displayName":"ismail mebsout","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GigU0PnKefD-R2lR5GVCQk1yGIr32KHGeWyGF1k=s64","userId":"12097835266700298787"}},"outputId":"59967bd9-c5ec-4e6b-df2b-9b40a7b57843"},"source":["import io\n","import re\n","import tqdm\n","import warnings\n","import itertools\n","from ast import literal_eval\n","\n","import numpy as np\n","import pandas as pd\n","\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","import plotly.express as px\n","\n","import nltk\n","import sklearn\n","from sklearn import decomposition\n","import tensorflow as tf\n","\n","dirpath=\"data/\"\n","\n","nltk.download(\"punkt\")\n","warnings.filterwarnings('ignore')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":115,"referenced_widgets":["5db48e768079410d8be1adf46bd1af13","6591e4d7d0834a53aa7ca5c670402f82","fe3c5c77c06248499ced64c7fd0fed04","0478d33c46314cf49a7ed79cb376098b","21bd1e83c6984416a8fe04412d08f897","15d4531052ee4f2c9941b98fcb391f86","604b3b9e4c2646dcadddfda6a1fbebb3","1163bbc4cbb84d4cbedbcf1ae3d4442f","53d3f0760c2d45238721a91bdb6fde07","04bc36de8c7c481cba7bacb044f8cd33","318ec404204748638afdb7d67382b75b","c0583191fa9540be85d27cefd11ad3b1","8cbc4c3c980645879de188a7b44cf14d","bb411be2f4ef4019985ca74babbe2aad","8308e7be8cd04e7cb9794fb259c37aa1","2126670a2a684c9bbbcc68fa4dfb275d"]},"id":"j-b9jcroXR-e","executionInfo":{"status":"ok","timestamp":1614589094460,"user_tz":-60,"elapsed":572,"user":{"displayName":"ismail mebsout","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GigU0PnKefD-R2lR5GVCQk1yGIr32KHGeWyGF1k=s64","userId":"12097835266700298787"}},"outputId":"6abb8358-42fe-443f-c5da-188aab34d5c1"},"source":["tqdm.tqdm_notebook()\n","tqdm.notebook.tqdm().pandas()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5db48e768079410d8be1adf46bd1af13","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"53d3f0760c2d45238721a91bdb6fde07","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"YfaL3WgeXR-f"},"source":["filepath = os.path.join(dirpath, \"clean_text_scrapped_data_2021.csv.gz\")\n","\n","DATASET_SIZE = 50000\n","def get_reviews(nrows=None):\n","    return pd.read_csv(filepath,\n","                     compression='gzip', \n","                     low_memory=False, \n","                     nrows=nrows,\n","                     parse_dates=['diner_date', 'rating_date'])\n","\n","\n","def clean_reviews(reviews):\n","    reviews['review'] = reviews.content.apply(lambda x: ' '.join(eval(x)))\n","    return reviews\n","\n","\n","def split_reviews_per_sentence(reviews):\n","    reviews[\"review_sentences\"] = reviews.review.progress_apply(\n","        lambda rvw: nltk.sent_tokenize(rvw)\n","    )\n","    return reviews"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":318},"id":"cOatV67RXR-g","executionInfo":{"status":"ok","timestamp":1614589109724,"user_tz":-60,"elapsed":2782,"user":{"displayName":"ismail mebsout","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GigU0PnKefD-R2lR5GVCQk1yGIr32KHGeWyGF1k=s64","userId":"12097835266700298787"}},"outputId":"fe2c56a1-1bdc-4d0c-f1d0-13ee69036f4e"},"source":["reviews = get_reviews(DATASET_SIZE)\n","reviews.head(2)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>0</th>\n","      <th>answer_text</th>\n","      <th>content</th>\n","      <th>diner_date</th>\n","      <th>id_comment</th>\n","      <th>id_resto</th>\n","      <th>other_ratings_category</th>\n","      <th>other_ratings_value</th>\n","      <th>rating</th>\n","      <th>rating_date</th>\n","      <th>resto</th>\n","      <th>resto_url</th>\n","      <th>reviewer_info_sup</th>\n","      <th>reviewer_origin</th>\n","      <th>reviewer_pseudo</th>\n","      <th>title</th>\n","      <th>url</th>\n","      <th>Day_of_week</th>\n","      <th>clean_content</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>482202</td>\n","      <td>NaN</td>\n","      <td>['We are extreamly sad about your Experiance. ...</td>\n","      <td>['We actually visited this place a few times f...</td>\n","      <td>2017-01-01</td>\n","      <td>g10259438-d11744624-r450260275</td>\n","      <td>g10259438-d11744624</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>1.0</td>\n","      <td>2017-01-07</td>\n","      <td>Buns_E17</td>\n","      <td>/Restaurant_Review-g10259438-d11744624-Reviews...</td>\n","      <td>[['pencil-paper', '11'], ['thumbs-up', '4']]</td>\n","      <td>['London, United Kingdom']</td>\n","      <td>lubix0209</td>\n","      <td>Really disappointing</td>\n","      <td>https://www.tripadvisor.com/ShowUserReviews-g1...</td>\n","      <td>5</td>\n","      <td>[\"'we\", 'actually', 'visited', 'place', 'time'...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>482231</td>\n","      <td>NaN</td>\n","      <td>['Hey gucci Burger', 'Thank you so much for th...</td>\n","      <td>['A blast of warm air greeted us as we entered...</td>\n","      <td>2019-01-01</td>\n","      <td>g10259438-d11744624-r645208007</td>\n","      <td>g10259438-d11744624</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>5.0</td>\n","      <td>2019-01-11</td>\n","      <td>Buns_E17</td>\n","      <td>/Restaurant_Review-g10259438-d11744624-Reviews...</td>\n","      <td>[['pencil-paper', '5'], ['thumbs-up', '1']]</td>\n","      <td>['Chingford, United Kingdom']</td>\n","      <td>Depeche242</td>\n","      <td>Gucci Burgers</td>\n","      <td>https://www.tripadvisor.com/ShowUserReviews-g1...</td>\n","      <td>4</td>\n","      <td>['blast', 'warm', 'air', 'greeted', 'u', 'ente...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Unnamed: 0  ...                                      clean_content\n","0      482202  ...  [\"'we\", 'actually', 'visited', 'place', 'time'...\n","1      482231  ...  ['blast', 'warm', 'air', 'greeted', 'u', 'ente...\n","\n","[2 rows x 20 columns]"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3Z_fV2NrXR-g","executionInfo":{"status":"ok","timestamp":1614557509409,"user_tz":-60,"elapsed":509,"user":{"displayName":"Johan Attia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtkiBeWvhK0qf7EYCzchGyI_qh-3XWiHmm5BJEGA=s64","userId":"00114683199204936957"}},"outputId":"d40d6fb8-8045-4a47-f049-b7e437800e79"},"source":["reviews.info()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 50000 entries, 0 to 49999\n","Data columns (total 20 columns):\n"," #   Column                  Non-Null Count  Dtype         \n","---  ------                  --------------  -----         \n"," 0   Unnamed: 0              50000 non-null  int64         \n"," 1   0                       0 non-null      float64       \n"," 2   answer_text             50000 non-null  object        \n"," 3   content                 50000 non-null  object        \n"," 4   diner_date              49064 non-null  datetime64[ns]\n"," 5   id_comment              50000 non-null  object        \n"," 6   id_resto                50000 non-null  object        \n"," 7   other_ratings_category  50000 non-null  object        \n"," 8   other_ratings_value     50000 non-null  object        \n"," 9   rating                  50000 non-null  float64       \n"," 10  rating_date             50000 non-null  datetime64[ns]\n"," 11  resto                   50000 non-null  object        \n"," 12  resto_url               50000 non-null  object        \n"," 13  reviewer_info_sup       50000 non-null  object        \n"," 14  reviewer_origin         50000 non-null  object        \n"," 15  reviewer_pseudo         50000 non-null  object        \n"," 16  title                   50000 non-null  object        \n"," 17  url                     50000 non-null  object        \n"," 18  Day_of_week             50000 non-null  int64         \n"," 19  clean_content           50000 non-null  object        \n","dtypes: datetime64[ns](2), float64(2), int64(2), object(14)\n","memory usage: 7.6+ MB\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"KRkKDOc3XR-g"},"source":["### Preprocessing & Understanding\n","\n","Let's visualize a review, contained in `content` column, then perform basic cleaning to get a proper text for each review"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bU0uCrDMXR-g","executionInfo":{"status":"ok","timestamp":1614557512452,"user_tz":-60,"elapsed":586,"user":{"displayName":"Johan Attia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtkiBeWvhK0qf7EYCzchGyI_qh-3XWiHmm5BJEGA=s64","userId":"00114683199204936957"}},"outputId":"12166852-d1a6-40b1-e627-914a3470e962"},"source":["reviews.content[0], type(reviews.content[0])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["('[\\'We actually visited this place a few times for lunch since it opened and it was fine. Good food, and staff are nice. \\', \\'We then got take away and it was awful. Late, cold, and trying to resolve the problem meant seven or eight phone calls before they finally refunded 50%. Pretty disappointing as we live locally and have always recommended it to friends etc. \\', \"Wood street needed a nice bar/restaurant and this place filled a void, but the terrible take out service coupled with how poorly we felt they handled the complaint makes me think twice about going again. Shame, as we\\'ve spent a lot of time and money here. \"]',\n"," str)"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":86},"id":"S6m4SJMqXR-h","executionInfo":{"status":"ok","timestamp":1614557517934,"user_tz":-60,"elapsed":1005,"user":{"displayName":"Johan Attia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtkiBeWvhK0qf7EYCzchGyI_qh-3XWiHmm5BJEGA=s64","userId":"00114683199204936957"}},"outputId":"4c6fd895-5f4c-4f43-8d21-43dc339c9e84"},"source":["reviews = clean_reviews(reviews)\n","reviews.review[0]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["\"We actually visited this place a few times for lunch since it opened and it was fine. Good food, and staff are nice.  We then got take away and it was awful. Late, cold, and trying to resolve the problem meant seven or eight phone calls before they finally refunded 50%. Pretty disappointing as we live locally and have always recommended it to friends etc.  Wood street needed a nice bar/restaurant and this place filled a void, but the terrible take out service coupled with how poorly we felt they handled the complaint makes me think twice about going again. Shame, as we've spent a lot of time and money here. \""]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"-8_s9TiVXR-h"},"source":["looks better now ! Now let's split every review by sentence using `sent_tokenize`from `nltk``"]},{"cell_type":"code","metadata":{"id":"FlBR1BY7XR-h"},"source":["reviews = split_reviews_per_sentence(reviews)\n","print(reviews[\"review_sentences\"][0], type(reviews[\"review_sentences\"][0]), sep=\"\\n\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"t9YKcsInXR-i"},"source":["For simplicity during this practical session, we will only consider the first *N* reviews. (you can make vary *N*)"]},{"cell_type":"code","metadata":{"id":"NEyXYstpXR-i"},"source":["first_reviews = reviews.head(20000)\n","first_reviews.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"X23VXAcZXR-i"},"source":["**Question** : plot the distribution of ratings. <br>\n","Rating will be the labels to predict in our classification modeling, so take care to distribution labels to define the good training and evaluation strategies."]},{"cell_type":"code","metadata":{"id":"EHs2JzAUXR-i"},"source":["### FILL THE BLANK ###"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SVPULm6fXR-i"},"source":["first_reviews[\"rating\"].value_counts() / len(first_reviews)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hpVikPaNXR-j"},"source":["This dataset is imbalanced, having ~50% of the dataset with rating = 5. The purpose of this notebook is not about validation metrics, but if we were to use accuracy, we should think about remediation strategy (e.g. subsampling the majority class). This imbalance issue could be one of the improvement axis for the homework."]},{"cell_type":"markdown","metadata":{"id":"_40i1E0V4Pz_"},"source":["\n","**Question** : plot the distribution of the number of sentences per review. <br>\n","When handling sequential/textual data, input length may differ from a review to another. In Deep Learning, knowing input length distribution is important to perform zero-padding or truncating processing."]},{"cell_type":"code","metadata":{"id":"ZH2Gw261XR-j"},"source":["### FILL THE BLANK ###"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C-NviD4u0fzO"},"source":["sentences = list(itertools.chain(*first_reviews[\"review_sentences\"]))\n","print(f\"Number of total sentences : {len(sentences)}\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tcK4m9ZU0hdS"},"source":["### Tokenization & Text Encoding\n","This part concerns tokenization and text encoding with TensorFlow modules :\n","\n","*(i) Build the token vocabulary* <br>\n","*(ii) Build a text encoder relying each word to an index, and thus each text to a sequence of word indices* (```list```) <br>\n","*(iii) Build a TensorFlow dataset for word2vec training*\n","\n"]},{"cell_type":"code","metadata":{"id":"clgWdmYXXR-j"},"source":["# Define and fit tokenizer\n","tokenizer = tf.keras.preprocessing.text.Tokenizer(filters=' ', char_level=False)\n","tokenizer.fit_on_texts(sentences)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MLRUO4TahOtY"},"source":["**Question** : use *texts_to_sequences* method of our tokenizer to get sequences from sentences."]},{"cell_type":"code","metadata":{"id":"w0HTCETChKl2"},"source":["sequences = ### FILL THE BLANK ###\n","print(first_reviews[\"review_sentences\"][0][0], sentences[0], sequences[0], sep=\"\\n\\n\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IE3p7FbchlX6"},"source":["**Question** : plot the distribution of the number of indices per sequence. <br>\n","For the same reasons as above, it is important to know length distribution of sentences/sequences."]},{"cell_type":"code","metadata":{"id":"-lAY2W09XR-k"},"source":["# Check the distribution of the number of indices per sequence\n","### FILL THE BLANK ###\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ro2yYodZXR-k"},"source":["# 1. Train our own word embeddings \n","\n","## Negative Sampling Skipgram\n","Preprocessing function which generates skip-gram pairs with negative sampling for a list of sequences (int-encoded sentences) based on window size, number of negative samples and (tokenizer) vocabulary size."]},{"cell_type":"code","metadata":{"id":"NGxcevNPXR-k"},"source":["max(tokenizer.index_word.keys())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L4YLQmzAXR-k"},"source":["## Generate training data for word2vec\n","\n","*(i) Define a sampling table for words in vocabulary, see [make_sampling_table](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/make_sampling_table)* <br>\n","*(ii) For each sequence (tokenized/indexed sentence), run sliding process (window) and appropriate word sampling to build positive skip-gram word pairs* <br>\n","*(iii) Iterate over each positive skip-gram pair to produce training examples with positive context word and negative samples, building corresponding labels.* <br>\n","*(iv) Returns overall combinations of (target word, context word, negative words)*"]},{"cell_type":"code","metadata":{"id":"zU3TWBxiXR-l"},"source":["\n","def generate_training_data(sequences, window_size, num_ns, vocab_size, seed=42):\n","    # Elements of each training example are appended to these lists.\n","    targets, contexts, labels = [], [], []\n","\n","    # Build the sampling table for vocab_size tokens.\n","    sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(vocab_size)\n","\n","    # Iterate over all sequences (sentences) in dataset.\n","    for sequence in tqdm.notebook.tqdm(sequences):\n","\n","        # Generate positive skip-gram pairs for a sequence (sentence).\n","        positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n","            sequence, \n","            vocabulary_size=vocab_size,\n","            sampling_table=sampling_table,\n","            window_size=window_size,\n","            negative_samples=0\n","        )\n","\n","        # Iterate over each positive skip-gram pair to produce training examples \n","        # with positive context word and negative samples.\n","        for target_word, context_word in positive_skip_grams:\n","            context_class = tf.expand_dims(tf.constant([context_word], dtype=\"int64\"), 1)\n","            negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n","                true_classes=context_class,\n","                num_true=1, \n","                num_sampled=num_ns, \n","                unique=True, \n","                range_max=vocab_size, \n","                seed=seed, \n","                name=\"negative_sampling\"\n","            )\n","\n","            # Build context and label vectors (for one target word)\n","            negative_sampling_candidates = tf.expand_dims(negative_sampling_candidates, 1)\n","\n","            context = tf.concat([context_class, negative_sampling_candidates], 0)\n","            label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\n","\n","            # Append each element from the training example to global lists.\n","            targets.append(target_word)\n","            contexts.append(context)\n","            labels.append(label)\n","\n","    return targets, contexts, labels"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mcdWsNb778TQ"},"source":["**Question** : Build training data using ```window_size=2``` and number of negative samples per positive pair ```num_ns=4```. "]},{"cell_type":"code","metadata":{"id":"IwgqAlDbXR-l"},"source":["targets, contexts, labels = generate_training_data(\n","    sequences=### FILL THE BLANK ###,\n","    window_size=2, \n","    num_ns=4, \n","    vocab_size=### FILL THE BLANK ### vocab size + 1 for padding\n",")\n","\n","print(len(targets), len(contexts), len(labels))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"My6BD4zjz32w"},"source":["targets[0], contexts[0]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ovE-2VTfXR-m"},"source":["## Define TensorFlow dataset\n","Define valid TensorFlow dataset from targets/contexts/labels iterable objects.\n","Set two parameters : \n","* *BUFFER_SIZE*\n","* *BATCH_SIZE*\n","\n","*BATCH_SIZE* can be particularly important for making training efficient. Note that *BATCH_SIZE* must be obviously lower than dataset size."]},{"cell_type":"code","metadata":{"id":"Scph45BfXR-m"},"source":["BATCH_SIZE = 1024\n","BUFFER_SIZE = 10000\n","\n","dataset = tf.data.Dataset.from_tensor_slices(((targets, contexts), labels))\n","dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n","print(dataset)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"S0TMNJXaXR-m"},"source":["Define Skipgram model"]},{"cell_type":"code","metadata":{"id":"bZA7iTnnXR-m"},"source":["class Skipgram(tf.keras.Model):\n","    \"\"\"Negative Sampling Skigpram implementation.\n","\n","    ```python\n","    w2v = Skipgram(vocab_size=4096, embedding_dim=128)\n","    ````\n","    \"\"\"\n","    def __init__(self, vocab_size, embedding_dim):\n","        \"\"\"Skigpram class constructor.\n","\n","        Parameters\n","        ----------\n","        vocab_size: int.\n","            Size of the vocabulary.\n","\n","        embedding_dim: int.\n","            Dimension of trained word2vec Skipgram embeddings.\n","\n","        \"\"\"\n","        super(Skipgram, self).__init__()\n","        self.target_embedding = tf.keras.layers.Embedding(\n","            vocab_size, \n","            embedding_dim,\n","            input_length=1,\n","            name=\"w2v_embedding\",\n","        )\n","        self.context_embedding = tf.keras.layers.Embedding(\n","            vocab_size, \n","            embedding_dim, \n","            input_length=4+1, # number of negative samples = 4\n","            name=\"context_embedding\",\n","        ) \n","        self.dots = tf.keras.layers.Dot(axes=(3,2))\n","        self.flatten = tf.keras.layers.Flatten()\n","\n","    def call(self, pair):\n","        \"\"\"Model forward method.\n","        \"\"\"\n","        target, context = pair\n","        we = self.target_embedding(target)\n","        ce = self.context_embedding(context)\n","        dots = self.dots([ce, we])\n","        \n","        return self.flatten(dots)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8dnLCjOPXR-m"},"source":["Define objective and training"]},{"cell_type":"code","metadata":{"id":"v-zUEfd1XR-n"},"source":["embedding_dim = 128\n","\n","word2vec = Skipgram(vocab_size=max(tokenizer.index_word.keys())+1, embedding_dim=128)\n","word2vec.compile(\n","    optimizer=\"adam\",\n","    loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n","    metrics=[\"accuracy\"]\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mQRseIwIXR-n"},"source":["word2vec.fit(dataset, epochs=20)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SxRlDbjSXR-n"},"source":["## Word Embeddings & Visualization"]},{"cell_type":"markdown","metadata":{"id":"tcTRrWCbXR-n"},"source":["word2vec.summary()"]},{"cell_type":"code","metadata":{"id":"8QhfXxzdXR-n"},"source":["pretrained_weights = word2vec.get_layer('w2v_embedding').get_weights()[0]\n","pretrained_weights.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sBNoZhO9XR-n"},"source":["Let's now visualize the embedding space, in 2 or 3 dimension. We could use a dimensionality reduction method such as PCA, T-SNE or UMAP.\n","\n","**Question** : after defining PCA object, use *fit_transform* method to get 3D-reduced vectors of word embeddings for visualization and print explained variance ratio."]},{"cell_type":"code","metadata":{"id":"m0CCxOq9XR-o"},"source":["def do_pca(pretrained_weights):\n","    pca = decomposition.PCA(n_components=3)\n","    reduced_weights = ### FILL THE BLANK ###\n","    return pca, reduced_weights\n","\n","pca, reduced_weights = do_pca(pretrained_weights)\n","print(reduced_weights.shape)\n","print(### FILL THE BLANK ###) # explained variance ratio"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ulGYQw6sXR-o"},"source":["df_pca = pd.DataFrame(data=reduced_weights, columns=[\"pca_1\", \"pca_2\", \"pca_3\"])\n","df_pca[\"word\"] = [\"<pad>\"] + list(tokenizer.word_index.keys())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TLMFvU7hXR-o"},"source":["fig = px.scatter_3d(\n","    df_pca, \n","    x=\"pca_1\", \n","    y=\"pca_2\", \n","    z=\"pca_3\",\n","    hover_name=\"word\",\n","    template=\"plotly_white\"\n",")\n","fig.update_layout(height=700, width=700)\n","fig.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lKkhvGHoi0lL"},"source":["**Question** : explore visual representations and similarities of pretrained word2vec embeddings using convenient visualizer. <br>\n","To get a better 3D representation, we can use the [TensorFlow Embedding Projector](https://projector.tensorflow.org/). To do that, we save word2vec embeddings (*vecs.tsv* + *meta.tsv*) as following to load them in the projector :"]},{"cell_type":"code","metadata":{"id":"BmvTACIskSpx"},"source":["out_v = io.open('vecs.tsv', 'w', encoding='utf-8')\n","out_m = io.open('meta.tsv', 'w', encoding='utf-8')\n","\n","for idx, word in tokenizer.index_word.items():\n","    \n","    vec = pretrained_weights[idx] # first idx is 1 : skip 0, it's padding.\n","    out_m.write(word + \"\\n\")\n","    out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n","    \n","out_v.close()\n","out_m.close()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pwX6qdbUXR-o"},"source":["## 2. Train a sentiment model \n","### Using your pre trained embeddings train a simple rating Classifier\n","\n","Our learned word embeddings can be used to represent the words of a text and to build a text representation. This text representation will be useful for classifcation.\n"]},{"cell_type":"code","metadata":{"id":"KOCekhToXR-o"},"source":["first_reviews[\"review_sentences\"].shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-3t4HsGXXR-p"},"source":["' '.join(first_reviews[\"review_sentences\"][0])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"i636CJTGmYbm"},"source":["**Question** : use *join* operator to get single string per review from lists in *review_sentences* column then plot the input length distribution of the sequences (tokenized reviews)."]},{"cell_type":"code","metadata":{"id":"QZ-c31wUXR-p"},"source":["processed_review = ### FILL THE BLANK ###\n","processed_sequences = tokenizer.texts_to_sequences(processed_review)\n","\n","### FILL THE BLANK ###\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4y_wdTGbXR-p"},"source":["padded_processed_sequences = tf.keras.preprocessing.sequence.pad_sequences(processed_sequences, maxlen=180, padding=\"post\")\n","padded_processed_sequences.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GJYSYpDGXR-p"},"source":["first_reviews.rating.value_counts()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"v53VhTXsXR-q"},"source":["first_reviews['usable_rating'] = first_reviews['rating'].apply(lambda r: int(r)-1)\n","first_reviews.usable_rating.value_counts()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1mLnXh11XR-q"},"source":["Build datasets"]},{"cell_type":"code","metadata":{"id":"w20AkIZLXR-q"},"source":["rating_labels = tf.keras.utils.to_categorical(first_reviews['usable_rating'], num_classes=5, dtype='float32')\n","X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(padded_processed_sequences, rating_labels, test_size=0.3)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1_Cj5xN_XR-q"},"source":["train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n","test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n","\n","train_ds = train_ds.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n","test_ds = test_ds.batch(BATCH_SIZE)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4xPEyNS5XR-q"},"source":["\n","**Simple rating Model architecture** is defined by :\n","1. An embedding layer initialized with pretrained word2vec embeddings.\n","2. A dense layer with 64 units without particular activation function for linear projection of the previous embedding vectors.\n","3. A global average pooling (1D).\n","4. A final dense layer for linear projection in a $d$-dimensional space for sentiment prediction, with $d$ the number sentiments/classes (here 5).\n"]},{"cell_type":"code","metadata":{"id":"8LHBjU_hXR-q"},"source":["simple_rating_model = tf.keras.Sequential([\n","    tf.keras.layers.Embedding(\n","        max(tokenizer.index_word.keys())+1, \n","        128, \n","        embeddings_initializer=tf.keras.initializers.Constant(pretrained_weights), \n","        trainable=True\n","    ),\n","    # Dense layer\n","    ### FILL THE BLANK ###\n","    \n","    # Global average pooling (NB: data_format='channels_last')\n","    ### FILL THE BLANK ###\n","    \n","    # Final dense layer (number of classes)\n","    ### FILL THE BLANK ###\n","    \n","])\n","\n","simple_rating_model.compile(\n","    loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n","    optimizer=\"adam\",\n","    metrics=[\"accuracy\"]\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LpIVaBJaXR-q"},"source":["Train model"]},{"cell_type":"code","metadata":{"id":"GTQJbNo7XR-r"},"source":["simple_history = simple_rating_model.fit(\n","    train_ds, \n","    epochs=20, \n","    validation_data=test_ds\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ScQyNSjrXR-r"},"source":["\n","## 3. Upgrade your sentiment model with attention mechanism through a Hierarchical Attention Network (HAN)\n","\n","<img src=\"https://drive.google.com/uc?id=1Q4H1MlBZ6ZasBCHxlARc25T9mBFRctiy\"/>\n","<ID of image>\n","\n","\n","Preprocessing\n","\n","For Hierarchical Attention Network, text preprocessing and inputs differ from the previous method. Here, the input is not just a sequence of all integers corresponding to all words/tokens of the review. Working at a sentence-level, the HAN input is a sequence of sequences (tokenized sentences) of integers. \n","\n","Thus, a such architecture needs a particular preprocessing (padding and truncating)."]},{"cell_type":"code","metadata":{"id":"_agH_yu0XR-r"},"source":["def review_preprocessing(review, words_maxlen=50, sentences_maxlen=10, tokenizer=tokenizer):\n","    \"\"\"Preprocessing function to build appropriate padded sequences for HAN.\n","\n","    Parameters\n","    ----------\n","    review: list.\n","        List of sentences (strings) of the review.\n","    \n","    words_maxlen: int.\n","        Maximal length/number of words for a sentence.\n","\n","    sentences_maxlen: int.\n","        Maximal length/number of sentences for a review.\n","\n","    Returns\n","    -------\n","    padded_sequences: tf.Tensor.\n","        Tensor of shape (sentences_maxlen, words_maxlen)\n","    \"\"\"\n","    sequences = tokenizer.texts_to_sequences(review)\n","    padded_sequences = tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=words_maxlen, padding=\"post\")\n","\n","    if padded_sequences.shape[0] < sentences_maxlen:\n","        padded_sequences = tf.pad(\n","            padded_sequences, \n","            paddings=tf.constant([[0, sentences_maxlen-padded_sequences.shape[0]], [0, 0]])\n","        )\n","    elif padded_sequences.shape[0] > sentences_maxlen:\n","        padded_sequences = padded_sequences[:sentences_maxlen]\n","\n","    assert padded_sequences.shape == (sentences_maxlen, words_maxlen)\n","    return padded_sequences"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2utg0mftXR-r"},"source":["review_preprocessing(first_reviews[\"review_sentences\"][0]).shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_SqJcZLmXR-r"},"source":["padded_preprocessed_reviews = [review_preprocessing(review) for review in tqdm.notebook.tqdm(first_reviews[\"review_sentences\"])]\n","padded_preprocessed_reviews = tf.stack(padded_preprocessed_reviews)\n","padded_preprocessed_reviews.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TTiFNJXwXR-s"},"source":["Build datasets"]},{"cell_type":"code","metadata":{"id":"mwmGPjLgXR-s"},"source":["X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(padded_preprocessed_reviews.numpy(), rating_labels, test_size=0.3)\n","print(\n","    X_train.shape,\n","    X_test.shape, \n","    y_train.shape, \n","    y_test.shape,\n","    sep=\"\\n\"\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MGRWD8jkXR-s"},"source":["train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n","test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n","\n","train_ds = train_ds.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n","test_ds = test_ds.batch(BATCH_SIZE)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yMF7X_-AXR-s"},"source":["Let's define our Attention Layer"]},{"cell_type":"code","metadata":{"id":"hAxP0Gj0XR-s"},"source":["class Attention(tf.keras.layers.Layer):\n","    \"\"\"Attention mechanism used in \"Hierarchical Attention Networks for Document Classification\" paper.\n","        \n","    ```python\n","    attention_layer = Attention(units=64)\n","    ```\n","    \"\"\"\n","    def __init__(self, units):\n","        \"\"\"Attention layer constructor.\n","\n","        Parameters\n","        ----------\n","        units: int.\n","            Dimension of the projection space.\n","        \"\"\"\n","        super(Attention, self).__init__()\n","        self.W = tf.keras.layers.Dense(units)\n","        self.u = tf.keras.layers.Dense(1)\n","\n","    def call(self, sequence):\n","        \"\"\"Layer forward method.\n","        \"\"\"\n","        attention_logits = self.u(tf.nn.tanh(self.W(sequence)))\n","        attention_weights = tf.nn.softmax(attention_logits)\n","\n","        weighted_vectors = attention_weights * sequence\n","        context_vector = tf.reduce_sum(weighted_vectors, axis=-2)\n","\n","        return context_vector, attention_weights"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KPOPpdrBXR-s"},"source":["batch1 = tf.random.normal((16, 10, 50, 128))\n","batch2 = tf.random.normal((16, 10, 128))\n","\n","attention = Attention(units=64)\n","\n","(att_batch1, weights_batch1), (att_batch2, weights_batch2) = attention(batch1), attention(batch2)\n","print(att_batch1.shape, weights_batch1.shape)\n","print(att_batch2.shape, weights_batch2.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2zcUCgbhXR-s"},"source":["**Hierarchical Attention Network architecture** is defined by:\n","\n","1. An embedding layer initialized with pretrained word2vec embeddings.\n","2. A sentence encoder : a Bidirectional GRU coupled with an Attention process run on words embeddings.\n","3. A document/review encoder : a Bidirectional GRU coupled with an Attention process run on sentence encoder outputs.\n","4. A final dense layer for linear projection in a $d$-dimensional space for sentiment prediction, with $d$ the number of possible sentiments/classes.\n"]},{"cell_type":"code","metadata":{"id":"qSGgVTp-XR-s"},"source":["class HierarchicalAttentionNetwork(tf.keras.Model):\n","    \"\"\"Hierarchical Attention Network implementation.\n","\n","    Reference :\n","    * Hierarchical Attention Networks for Document Classification : https://www.cs.cmu.edu/~./hovy/papers/16HLT-hierarchical-attention-networks.pdf\n","\n","    \"\"\"\n","    def __init__(self, vocab_size, embedding_dim, gru_units, attention_units, classifier_units, pretrained_weights=None):\n","        \"\"\"Hierarchical Attention Network class constructor.\n","\n","        \"\"\"\n","        super(HierarchicalAttentionNetwork, self).__init__()\n","        \n","        if pretrained_weights is not None:\n","            initializer = tf.keras.initializers.Constant(pretrained_weights)\n","        else:\n","            initializer = \"uniform\"\n","\n","        self.embedding = tf.keras.layers.Embedding(\n","            vocab_size, \n","            embedding_dim, \n","            embeddings_initializer=initializer,\n","            trainable=True\n","        )\n","        self.WordGRU = tf.keras.layers.Bidirectional(\n","            tf.keras.layers.GRU(\n","                units=gru_units,\n","                activation=\"tanh\",\n","                return_sequences=True\n","            ), \n","            merge_mode='concat',\n","        )\n","        self.WordAttention = Attention(units=attention_units)\n","        self.SentenceGRU = tf.keras.layers.Bidirectional(\n","            tf.keras.layers.GRU(\n","                units=gru_units,\n","                activation=\"tanh\",\n","                return_sequences=True\n","            ), \n","            merge_mode='concat',\n","        )\n","        self.SentenceAttention = Attention(units=attention_units)\n","        self.fc = tf.keras.layers.Dense(units=classifier_units)\n","\n","    def call(self, x):\n","        \"\"\"Model forward method.\n","        \"\"\"\n","        sentences_vectors, _ = self.word_to_sentence_encoder(x)\n","        document_vector, _ = self.sentence_to_document_encoder(sentences_vectors)\n","        return self.fc(document_vector)\n","\n","    def word_to_sentence_encoder(self, x):\n","        \"\"\"Given words from each sentences, encode the contextual representation of \n","        the words from the sentence with Bidirectional GRU and Attention, and output \n","        a sentence_vector.\n","        \"\"\"\n","        x = self.embedding(x)\n","        x = tf.keras.layers.TimeDistributed(self.WordGRU)(x)\n","        context_vector, attention_weights = self.WordAttention(x)\n","\n","        return context_vector, attention_weights\n","    \n","    def sentence_to_document_encoder(self, sentences_vectors):\n","        \"\"\"Given sentences from each review, encode the contextual representation of \n","        the sentences with Bidirectional GRU and Attention, and output \n","        a document vector.\n","        \"\"\"\n","        # sentence encoder (using bidirectionnal GRU)\n","        ### FILL THE BLANK ###\n","        \n","        # document vector  (using attention at sentence level)\n","        ### FILL THE BLANK ###\n","        \n","        return document_vector, attention_weights"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OsRWLTtbXR-u"},"source":["han_model = HierarchicalAttentionNetwork(\n","    vocab_size=max(tokenizer.index_word.keys())+1, \n","    embedding_dim=128, \n","    pretrained_weights=pretrained_weights, \n","    gru_units=32, \n","    attention_units=32, \n","    classifier_units=5\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TzAKegZYXR-v"},"source":["test_batch = tf.zeros((256, 10, 50))\n","test_output = han_model(test_batch)\n","test_output.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0WPs14m1H1Am"},"source":["**Question** : Compile and train HAN model in an analoguous way than previous simple model"]},{"cell_type":"code","metadata":{"id":"fLes0rYSXR-v"},"source":["han_model.compile(\n","    ### FILL THE BLANK ###,\n","    ### FILL THE BLANK ###,\n","    ### FILL THE BLANK ###\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9mTrUhmYXR-v"},"source":["han_history = han_model.fit(\n","    ### FILL THE BLANK ###,\n","    ### FILL THE BLANK ###,\n","    ### FILL THE BLANK ###\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BlCxOxYbXR-v"},"source":["*Conclusion* :\n","* The simple model is an efficient baseline (good performance and pretty fast training)\n","* HAN model is much more heavy, with stronger and powerful learning/mapping capabilities for training data, involving overfitting risk. Thus it needs regularization (e.g. dropout for instance) for generalization.\n","\n","## Some improvements for hands-on and homework \n","**Questions : Consider some (1 or 2) of these suggestions to improve HAN performances** :\n","* Regularization for generalization : dropout, recurrent dropout, L2/L1 regularization\n","* Address unbalacanced data problems : \n","  * For training : oversampling, subsampling, loss weighting, new loss function ?\n","  * For evaluation : consider other metrics than accuracy : precision, recall, f1-score, confusion matrix...\n","* Aggressive padding strategy : introduce masking ?\n","* Iterate with more data, including more validation data"]},{"cell_type":"code","metadata":{"id":"q1_FWygKO_7D"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vNcO-IWNO_z_"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uHinbXbmO_p6"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"O88QLJldXR-w"},"source":["# 4. Visualize attention weights to interpet main sentences and words involved in the prediction\n","\n","**Question 1** : add a new ```document_encoder``` method in HierarchicalAttentionNetwork model which output the document vector and attention weights (sentences weighting) from preprocessed review to determine the most importance sentences. \n","\n","**Question 2** : get word attention weights for each sentence of a review to show the important words of important sentences."]},{"cell_type":"code","metadata":{"id":"JDzVm5JHXR-w"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DKMOOUcjXR-w"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VfqfEb0mXR-w"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AUHygYH3XR-w"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q65BsgNMXR-w"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AfWYGhwzXR-x"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6DHo62-aXR-x"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bcsQbxP4XR-x"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GzRAt_PzXR-x"},"source":[""],"execution_count":null,"outputs":[]}]}